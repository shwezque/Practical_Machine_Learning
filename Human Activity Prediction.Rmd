---
title: "Human Activity Prediction"
author: "Shaun Wesley Que"
date: "April 22, 2016"
output: html_document
---

## Introduction
Using devices such as *Jawbone Up*, *Nike FuelBand*, and *Fitbit*, it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how *much* of a particular activity they do, but they rarely quantify *how well they do it*. In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. Moree information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Data
The project assignment includes two data files (in csv format), that can be downloaded from these links:

1. [Training Data: pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)
2. [Testing Data: pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.

## Goal
The goal of the project is to predict the manner in which the subjects did the exercise. This is the "classe" variable in the training set. This project includes a report on how the model is built, how cross validation is used, the expected out of sample error, and an explanation for each of the choices. The prediction model will be used to predict 20 different test cases.

## Report
### Data Preparation
```{r}
library(caret)
if(!file.exists("pml-training.csv")){
    download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "pml-training.csv")
}
if(!file.exists("pml-testing.csv")){
    download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = "pml-testing.csv")
}
ptrain <- read.csv("pml-training.csv")
ptest <- read.csv("pml-testing.csv")
```

To estimate the out-of-sample error, I will randomly split the full training data (*ptrain*) into a smaller training set (*ptrain1*) and a validation set (*ptrain2*):
```{r}
set.seed(314159)
inTrain <- createDataPartition(y=ptrain$classe, p=0.7, list=FALSE)
ptrain1 <- ptrain[inTrain,]
ptrain2 <- ptrain[-inTrain,]
```

Now I am going to reduce the number of features by removing the following variables:

1. Variables with nearly zero variance
2. Variables that are almost always NA
3. Variables that do not make intuitive sense for prediction

These removals are based on the dataset in ptrain1, and are afterwards applied to ptrain2.
```{r}
# 1. Remove variables with nearly zero variance
nzv <- nearZeroVar(ptrain1)
ptrain1 <- ptrain1[, -nzv]
ptrain2 <- ptrain2[, -nzv]

# 2. Remove variables that are almost always NA
mostlyNA <- sapply(ptrain1, function(x) mean(is.na(x))) > 0.95
ptrain1 <- ptrain1[, mostlyNA==F]
ptrain2 <- ptrain2[, mostlyNA==F]

# 3. Remove variables that do not make intuitive sense for prediction
ptrain1 <- ptrain1[, -(1:5)]
ptrain2 <- ptrain2[, -(1:5)]
```

### Model Building
I decided to start with a Random Forest model on ptrain1, and instruct the "train" function to use a 3-fold cross-validation to select the optimal tuning parameters for the model.
```{r}
fitControl <- trainControl(method="cv", number=3, verboseIter=F)
fit <- train(classe~., data=ptrain1, method="rf", trControl=fitControl)
fit$finalModel
```

From the results, it can be seen that the model used 500 trees and tried 27 variables at each split. 

### Cross-Validation
Now, I used the fitted model to predict `classe` in ptrain2, and show the confusion matrix to compare the predicted versus the actual labels:
```{r} 
preds <- predict(fit, newdata=ptrain2)
confusionMatrix(ptrain2$classe, preds)
```

The accuracy is `r round(confusionMatrix(ptrain2$classe, preds)$overall[1], 3)*100`%, which means that the predicted accuracy for the out-of-sample error is `r round(1-confusionMatrix(ptrain2$classe, preds)$overall[1], 3)*100`%. 

### Re-training the Selected Model
Before predicting on the test set, it is  importat to train the model on the full training set (ptrain), rather than just ptrain1. This will produced a more accurate prediction:
```{r}
# 1. Remove variables with nearly zero variance
nzv <- nearZeroVar(ptrain)
ptrain <- ptrain[, -nzv]
ptest <- ptest[, -nzv]

# 2. Remove variables that are almost always NA
mostlyNA <- sapply(ptrain, function(x) mean(is.na(x))) > 0.95
ptrain <- ptrain[, mostlyNA==F]
ptest <- ptest[, mostlyNA==F]

# 3. Remove variables that do not make intuitive sense for prediction
ptrain <- ptrain[, -(1:5)]
ptest <- ptest[, -(1:5)]

# Re-fit the model using full training set
fitControl <- trainControl(method="cv", number=3, verboseIter=F)
fit <- train(classe~., data=ptrain, method="rf", trControl=fitControl)
```

### Predicting the Testing Set
I now use the model fit on ptrain to predict the label for the observations in ptest:
```{r}
# prediction on test set
preds <- predict(fit, newdata=ptest)

# convert predictions to character vector
preds <- as.character(preds)

# create function to write predictions to files
pml_write_files <- function(x) {
    n <- length(x)
    for(i in 1:n) {
        filename <- paste0("problem_id_", i, ".txt")
        write.table(x[i], file=filename, quote=F, row.names=F, col.names=F)
    }
}

# create prediction files to submit
pml_write_files(preds)
```
